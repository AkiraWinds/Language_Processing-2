{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Preprocessing"
      ],
      "metadata": {
        "id": "X7FYjipSbjSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHveANePwksf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 数据预处理函数\n",
        "def preprocess_text(text):\n",
        "    text = text.replace('\\n', ' ')  # 替换段落中的换行符为一个空格\n",
        "    text = re.sub(r'@\\w+', '<@>', text)  # 替换提及的用户\n",
        "    text = re.sub(r'#\\w+', '<#>', text)  # 替换标签\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', 'U', text)  # 替换URL\n",
        "    text = re.sub(r'\\b\\d+\\b', 'N', text)  # 替换独立的数字\n",
        "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', 'D', text)  # 替换日期\n",
        "    text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', 'T', text)  # 替换时间\n",
        "    text = re.sub(r'<.*?>', '', text)  # 移除HTML标签\n",
        "    text = re.sub(r'[^\\w\\s<>.,!?]', '', text)  # 移除非字母数字字符\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # 替换多个空格为单个空格\n",
        "    text = text.lower()  # 转换为小写\n",
        "    return text\n",
        "\n",
        "def check_files(input_dir):\n",
        "    # 获取所有txt文件和对应的json文件\n",
        "    txt_files = [f for f in os.listdir(input_dir) if f.startswith('problem-') and f.endswith('.txt')]\n",
        "    file_pairs = [(os.path.join(input_dir, txt_file), os.path.join(input_dir, txt_file.replace('problem-', 'truth-problem-').replace('.txt', '.json'))) for txt_file in txt_files]\n",
        "\n",
        "    for txt_file, json_file in tqdm(file_pairs, desc='Checking files'):\n",
        "        try:\n",
        "            file_id = int(os.path.basename(txt_file).split('-')[1].split('.')[0])  # 提取文件ID\n",
        "\n",
        "            with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "                paragraphs = f.read().strip().split('\\n')\n",
        "                paragraphs = [para.strip() for para in paragraphs if para.strip()]  # 去除空段落\n",
        "\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                truth_data = json.load(f)\n",
        "                author_changes = truth_data['changes']\n",
        "\n",
        "            if len(paragraphs) != len(author_changes) + 1:\n",
        "                print(f\"Mismatch in file {txt_file}: {len(paragraphs)} paragraphs, {len(author_changes)} author changes\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {txt_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_files(input_dir, output_dir, batch_size=100):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 获取所有txt文件和对应的json文件\n",
        "    txt_files = [f for f in os.listdir(input_dir) if f.startswith('problem-') and f.endswith('.txt')]\n",
        "    file_pairs = [(os.path.join(input_dir, txt_file), os.path.join(input_dir, txt_file.replace('problem-', 'truth-problem-').replace('.txt', '.json'))) for txt_file in txt_files]\n",
        "\n",
        "    for batch_idx in range(0, len(file_pairs), batch_size):\n",
        "        batch_data = []\n",
        "        batch_files = file_pairs[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "        for txt_file, json_file in tqdm(batch_files, desc=f'Processing batch {batch_idx // batch_size + 1}'):\n",
        "            try:\n",
        "                file_id = int(os.path.basename(txt_file).split('-')[1].split('.')[0])  # 提取文件ID\n",
        "\n",
        "                with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "                    paragraphs = f.read().strip().split('\\n')\n",
        "                    paragraphs = [para.strip() for para in paragraphs if para.strip()]  # 去除空段落\n",
        "\n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    truth_data = json.load(f)\n",
        "                    author_changes = truth_data['changes']\n",
        "\n",
        "                if len(paragraphs) != len(author_changes) + 1:\n",
        "                    print(f\"Mismatch in file {txt_file}: {len(paragraphs)} paragraphs, {len(author_changes)} author changes\")\n",
        "                    continue\n",
        "\n",
        "                for i, para in enumerate(paragraphs):\n",
        "                    processed_para = preprocess_text(para)\n",
        "                    data = {\n",
        "                        'file_id': file_id,\n",
        "                        'paragraph_index': i,\n",
        "                        'text': processed_para,\n",
        "                        'author_change': author_changes[i - 1] if i > 0 else None\n",
        "                    }\n",
        "                    batch_data.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {txt_file}: {e}\")\n",
        "                print(f\"Number of paragraphs: {len(paragraphs)}\")\n",
        "                print(f\"Number of author changes: {len(author_changes)}\")\n",
        "        # 保存中间结果\n",
        "        if batch_data:\n",
        "            intermediate_df = pd.DataFrame(batch_data)\n",
        "            intermediate_df.to_csv(os.path.join(output_dir, f'batch_{batch_idx // batch_size}.csv'), index=False)\n",
        "\n",
        "def combine_batches(output_dir, final_output_file):\n",
        "    batch_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith('batch_') and f.endswith('.csv')]\n",
        "    all_data = []\n",
        "\n",
        "    for batch_file in tqdm(batch_files, desc='Combining batches'):\n",
        "        df = pd.read_csv(batch_file)\n",
        "        all_data.append(df)\n",
        "\n",
        "    final_df = pd.concat(all_data, ignore_index=True)\n",
        "    final_df.to_csv(final_output_file, index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s0TGoyfwyXPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_dir = './pan24-multi-author-analysis/easy/train'\n",
        "output_dir = './pan24-multi-author-analysis/easy/processed_train'\n",
        "process_files(input_dir, output_dir)\n",
        "\n",
        "final_output_file = './pan24-multi-author-analysis/easy/easy_train_dataframe.csv'\n",
        "combine_batches(output_dir, final_output_file)"
      ],
      "metadata": {
        "id": "gPuGMz3Qycym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Feature Extraction"
      ],
      "metadata": {
        "id": "Heqfe4Ui_peL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 feature extraction for training datasets"
      ],
      "metadata": {
        "id": "UdipKNSfcYaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the feature extraction functions\n",
        "def calculate_clause_density(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    clause_indicators = [',', 'but', 'or', 'that', 'if', 'whether', 'whom', 'whose', 'how', 'why', 'because', 'as', 'once', 'though', 'although']\n",
        "    clause_count = sum(text.count(indicator) for indicator in clause_indicators) + len(sentences)\n",
        "    return clause_count / len(sentences) if sentences else 0\n",
        "\n",
        "def calculate_punctuation_density(text):\n",
        "    punctuation_marks = re.findall(r'[,;.!?:\\\"\\'\\-—(){}\\[\\]]', text)\n",
        "    return len(punctuation_marks) / len(text.split()) if text.split() else 0\n",
        "\n",
        "def calculate_syllables_per_word(text):\n",
        "    words = word_tokenize(text)\n",
        "    syllable_count = sum(len(re.findall(r'[aeiouy]', word.lower())) for word in words)\n",
        "    return syllable_count / len(words) if words else 0\n",
        "\n",
        "def calculate_sentence_length(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_count = sum(len(sentence.split()) for sentence in sentences)\n",
        "    return word_count / len(sentences) if sentences else 0\n",
        "\n",
        "def calculate_noun_complexity(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    noun_count = sum(1 for word, pos in tagged_words if pos.startswith('NN'))\n",
        "    return noun_count / len(words) if words else 0\n",
        "\n",
        "def extract_features_and_labels_from_dataframe(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    grouped = data.groupby(['file_id', 'difficulty'])\n",
        "\n",
        "    for (file_id, difficulty), group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                clause_density = calculate_clause_density(para)\n",
        "                punctuation_density = calculate_punctuation_density(para)\n",
        "                syllables_per_word = calculate_syllables_per_word(para)\n",
        "                sentence_length = calculate_sentence_length(para)\n",
        "                noun_complexity = calculate_noun_complexity(para)\n",
        "\n",
        "                prev_clause_density = calculate_clause_density(prev_para)\n",
        "                prev_punctuation_density = calculate_punctuation_density(prev_para)\n",
        "                prev_syllables_per_word = calculate_syllables_per_word(prev_para)\n",
        "                prev_sentence_length = calculate_sentence_length(prev_para)\n",
        "                prev_noun_complexity = calculate_noun_complexity(prev_para)\n",
        "\n",
        "                features.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "                    'clause_density': clause_density,\n",
        "                    'punctuation_density': punctuation_density,\n",
        "                    'syllables_per_word': syllables_per_word,\n",
        "                    'sentence_length': sentence_length,\n",
        "                    'noun_complexity': noun_complexity,\n",
        "                    'prev_clause_density': prev_clause_density,\n",
        "                    'prev_punctuation_density': prev_punctuation_density,\n",
        "                    'prev_syllables_per_word': prev_syllables_per_word,\n",
        "                    'prev_sentence_length': prev_sentence_length,\n",
        "                    'prev_noun_complexity': prev_noun_complexity\n",
        "                })\n",
        "\n",
        "                labels.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "                    'author_change': group['author_change'].iloc[i]\n",
        "                })\n",
        "\n",
        "    feature_columns = [\n",
        "        'file_id', 'paragraph_index', 'difficulty', 'clause_density', 'punctuation_density', 'syllables_per_word',\n",
        "        'sentence_length', 'noun_complexity', 'prev_clause_density', 'prev_punctuation_density',\n",
        "        'prev_syllables_per_word', 'prev_sentence_length', 'prev_noun_complexity'\n",
        "    ]\n",
        "\n",
        "    label_columns = ['file_id', 'paragraph_index', 'difficulty', 'author_change']\n",
        "\n",
        "    return pd.DataFrame(features, columns=feature_columns), pd.DataFrame(labels, columns=label_columns)\n",
        "\n",
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('lp/merged_train_data.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df, labels_df = extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('combined_train_features.csv', index=False)\n",
        "labels_df.to_csv('combined_train_labels.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n",
        "print(\"Labels saved\")\n"
      ],
      "metadata": {
        "id": "WZ-DgInBCHx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 extracting POS density, dependency parsing features... and the rest of the features."
      ],
      "metadata": {
        "id": "qH_gxphCckwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pos\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the feature extraction functions\n",
        "def calculate_pos_density(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = {\n",
        "        'NOUN': 0,\n",
        "        'VERB': 0,\n",
        "        'ADJ': 0,\n",
        "        'ADV': 0,\n",
        "    }\n",
        "    for token in doc:\n",
        "        if token.pos_ in pos_counts:\n",
        "            pos_counts[token.pos_] += 1\n",
        "\n",
        "    total_words = len(doc) if len(doc) > 0 else 1  # Avoid division by zero\n",
        "    for key in pos_counts:\n",
        "        pos_counts[key] /= total_words\n",
        "\n",
        "    return pos_counts\n",
        "\n",
        "def extract_features_and_labels_from_dataframe(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    grouped = data.groupby(['file_id', 'difficulty'])\n",
        "\n",
        "    for (file_id, difficulty), group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                pos_density = calculate_pos_density(para)\n",
        "\n",
        "                prev_pos_density = calculate_pos_density(prev_para)\n",
        "\n",
        "                features.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "\n",
        "                    'pos_noun_density': pos_density['NOUN'],\n",
        "                    'pos_verb_density': pos_density['VERB'],\n",
        "                    'pos_adj_density': pos_density['ADJ'],\n",
        "                    'pos_adv_density': pos_density['ADV'],\n",
        "                    'prev_pos_noun_density': prev_pos_density['NOUN'],\n",
        "                    'prev_pos_verb_density': prev_pos_density['VERB'],\n",
        "                    'prev_pos_adj_density': prev_pos_density['ADJ'],\n",
        "                    'prev_pos_adv_density': prev_pos_density['ADV']\n",
        "                })\n",
        "\n",
        "                labels.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "                    'author_change': group['author_change'].iloc[i]\n",
        "                })\n",
        "\n",
        "    feature_columns = [\n",
        "        'file_id', 'paragraph_index', 'difficulty', 'pos_noun_density', 'pos_verb_density', 'pos_adj_density', 'pos_adv_density',\n",
        "        'prev_noun_complexity', 'prev_pos_noun_density', 'prev_pos_verb_density', 'prev_pos_adj_density', 'prev_pos_adv_density'\n",
        "    ]\n",
        "\n",
        "    label_columns = ['file_id', 'paragraph_index', 'difficulty', 'author_change']\n",
        "\n",
        "    return pd.DataFrame(features, columns=feature_columns), pd.DataFrame(labels, columns=label_columns)\n",
        "\n",
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('lp/merged_train_data.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df, labels_df = extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('pos.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n"
      ],
      "metadata": {
        "id": "4hW-LVN34NKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_dependency_parsing_features(text):\n",
        "    dependency_features = [0, 0, 0, 0, 0, 0, 0]  # ['nsubj', 'pobj', 'dobj', 'ROOT', 'neg', 'aux', 'conj']\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'nsubj':\n",
        "            dependency_features[0] += 1\n",
        "        elif token.dep_ == 'pobj':\n",
        "            dependency_features[1] += 1\n",
        "        elif token.dep_ == 'dobj':\n",
        "            dependency_features[2] += 1\n",
        "        elif token.dep_ == 'ROOT':\n",
        "            dependency_features[3] += 1\n",
        "        elif token.dep_ == 'neg':\n",
        "            dependency_features[4] += 1\n",
        "        elif token.dep_ == 'aux':\n",
        "            dependency_features[5] += 1\n",
        "        elif token.dep_ == 'conj':\n",
        "            dependency_features[6] += 1\n",
        "\n",
        "    total_words = len(doc) if len(doc) > 0 else 1\n",
        "    dependency_features = [count / total_words for count in dependency_features]\n",
        "\n",
        "    return dependency_features\n",
        "\n",
        "\n",
        "def extract_features_and_labels_from_dataframe(data):\n",
        "    features = []\n",
        "\n",
        "    grouped = data.groupby(['file_id', 'difficulty'])\n",
        "\n",
        "    for (file_id, difficulty), group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "\n",
        "                dependency_features = calculate_dependency_parsing_features(para)\n",
        "                prev_dependency_features = calculate_dependency_parsing_features(prev_para)\n",
        "                features.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "                    'dependency': dependency_features,\n",
        "                    'prev_dependency': prev_dependency_features,\n",
        "                })\n",
        "\n",
        "\n",
        "\n",
        "    feature_columns = [\n",
        "        'file_id', 'paragraph_index', 'difficulty', 'dependency', 'prev_dependency']\n",
        "    return pd.DataFrame(features, columns=feature_columns)\n",
        "\n",
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('lp/merged_train_data.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df = extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('dp_features.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EvYvigRT_ZNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the rest\n",
        "\n",
        "# Define the new feature extraction functions\n",
        "def calculate_function_word_frequencies(text):\n",
        "    function_words = [0, 0, 0, 0]  # [pronouns, prepositions, conjunctions, articles]\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PRON':\n",
        "            function_words[0] += 1\n",
        "        elif token.pos_ == 'ADP':\n",
        "            function_words[1] += 1\n",
        "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
        "            function_words[2] += 1\n",
        "        elif token.pos_ == 'DET':\n",
        "            function_words[3] += 1\n",
        "\n",
        "    total_words = len(doc) if len(doc) > 0 else 1\n",
        "    function_words = [count / total_words for count in function_words]\n",
        "\n",
        "    return function_words\n",
        "\n",
        "def calculate_vocabulary_diversity(text):\n",
        "    words = word_tokenize(text)\n",
        "    unique_words = set(words)\n",
        "    return len(unique_words) / len(words) if len(words) > 0 else 0\n",
        "\n",
        "def calculate_sentence_length_variety(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    lengths = [len(sentence.split()) for sentence in sentences]\n",
        "    return pd.Series(lengths).var() if len(lengths)>1 else 0\n",
        "\n",
        "def extract_features_and_labels_from_dataframe(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    grouped = data.groupby(['file_id', 'difficulty'])\n",
        "\n",
        "    for (file_id, difficulty), group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                # New features\n",
        "                function_word_frequencies = calculate_function_word_frequencies(para)\n",
        "                vocabulary_diversity = calculate_vocabulary_diversity(para)\n",
        "                sentence_length_variety = calculate_sentence_length_variety(para)\n",
        "\n",
        "                prev_function_word_frequencies = calculate_function_word_frequencies(prev_para)\n",
        "                prev_vocabulary_diversity = calculate_vocabulary_diversity(prev_para)\n",
        "                prev_sentence_length_variety = calculate_sentence_length_variety(prev_para)\n",
        "\n",
        "                features.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "                    'function_word_frequencies': function_word_frequencies,\n",
        "                    'vocabulary_diversity': vocabulary_diversity,\n",
        "                    'sentence_length_variety': sentence_length_variety,\n",
        "                    'prev_function_word_frequencies': prev_function_word_frequencies,\n",
        "                    'prev_vocabulary_diversity': prev_vocabulary_diversity,\n",
        "                    'prev_sentence_length_variety': prev_sentence_length_variety\n",
        "                })\n",
        "\n",
        "                labels.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'difficulty': group['difficulty'].iloc[i],\n",
        "                    'author_change': group['author_change'].iloc[i]\n",
        "                })\n",
        "\n",
        "    feature_columns = [\n",
        "        'file_id', 'paragraph_index', 'difficulty', 'function_word_frequencies',\n",
        "        'vocabulary_diversity', 'sentence_length_variety', 'prev_function_word_frequencies', 'prev_vocabulary_diversity', 'prev_sentence_length_variety'\n",
        "    ]\n",
        "\n",
        "    label_columns = ['file_id', 'paragraph_index', 'difficulty', 'author_change']\n",
        "\n",
        "    return pd.DataFrame(features, columns=feature_columns), pd.DataFrame(labels, columns=label_columns)\n",
        "\n",
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('lp/merged_train_data.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df, labels_df = extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('combined_train_features2.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0rgDCypUmn5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 feature extraction for validation datasets"
      ],
      "metadata": {
        "id": "BZ3agn_jcyP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def val_extract_features_and_labels_from_dataframe(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    grouped = data.groupby(['file_id'])\n",
        "\n",
        "    for file_id, group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                clause_density = calculate_clause_density(para)\n",
        "                punctuation_density = calculate_punctuation_density(para)\n",
        "                syllables_per_word = calculate_syllables_per_word(para)\n",
        "                sentence_length = calculate_sentence_length(para)\n",
        "                noun_complexity = calculate_noun_complexity(para)\n",
        "\n",
        "                prev_clause_density = calculate_clause_density(prev_para)\n",
        "                prev_punctuation_density = calculate_punctuation_density(prev_para)\n",
        "                prev_syllables_per_word = calculate_syllables_per_word(prev_para)\n",
        "                prev_sentence_length = calculate_sentence_length(prev_para)\n",
        "                prev_noun_complexity = calculate_noun_complexity(prev_para)\n",
        "\n",
        "                features.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'clause_density': clause_density,\n",
        "                    'punctuation_density': punctuation_density,\n",
        "                    'syllables_per_word': syllables_per_word,\n",
        "                    'sentence_length': sentence_length,\n",
        "                    'noun_complexity': noun_complexity,\n",
        "                    'prev_clause_density': prev_clause_density,\n",
        "                    'prev_punctuation_density': prev_punctuation_density,\n",
        "                    'prev_syllables_per_word': prev_syllables_per_word,\n",
        "                    'prev_sentence_length': prev_sentence_length,\n",
        "                    'prev_noun_complexity': prev_noun_complexity\n",
        "                })\n",
        "\n",
        "                labels.append({\n",
        "                    'file_id': group['file_id'].iloc[i],\n",
        "                    'paragraph_index': group['paragraph_index'].iloc[i],\n",
        "                    'author_change': group['author_change'].iloc[i]\n",
        "                })\n",
        "\n",
        "    feature_columns = [\n",
        "        'file_id', 'paragraph_index', 'clause_density', 'punctuation_density', 'syllables_per_word',\n",
        "        'sentence_length', 'noun_complexity', 'prev_clause_density', 'prev_punctuation_density',\n",
        "        'prev_syllables_per_word', 'prev_sentence_length', 'prev_noun_complexity'\n",
        "    ]\n",
        "\n",
        "    label_columns = ['file_id', 'paragraph_index', 'author_change']\n",
        "\n",
        "    return pd.DataFrame(features, columns=feature_columns), pd.DataFrame(labels, columns=label_columns)\n",
        "\n",
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('./pan24-multi-author-analysis/easy/easy_validation_dataframe.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df, labels_df = val_extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('./pan24-multi-author-analysis/validation/easy_validation_features.csv', index=False)\n",
        "labels_df.to_csv('./pan24-multi-author-analysis/validation/easy_validation_labels.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n",
        "print(\"Labels saved\")\n"
      ],
      "metadata": {
        "id": "b48Q4zu7c1GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('./pan24-multi-author-analysis/medium/medium_validation_dataframe.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df, labels_df = extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('./pan24-multi-author-analysis/validation/medium_validation_features.csv', index=False)\n",
        "labels_df.to_csv('./pan24-multi-author-analysis/validation/medium_validation_labels.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n",
        "print(\"Labels saved\")\n",
        "\n",
        "# Load the merged DataFrame\n",
        "merged_data = pd.read_csv('./pan24-multi-author-analysis/hard/hard_validation_dataframe.csv')\n",
        "\n",
        "# Extract features and labels from the merged DataFrame\n",
        "features_df, labels_df = extract_features_and_labels_from_dataframe(merged_data)\n",
        "\n",
        "# Save the features and labels to separate CSV files\n",
        "features_df.to_csv('./pan24-multi-author-analysis/validation/hard_validation_features.csv', index=False)\n",
        "labels_df.to_csv('./pan24-multi-author-analysis/validation/hard_validation_labels.csv', index=False)\n",
        "\n",
        "print(\"Features saved\")\n",
        "print(\"Labels saved\")"
      ],
      "metadata": {
        "id": "Fl9sUtw9dk4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the feature extraction function\n",
        "def calculate_pos_density(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = {\n",
        "        'NOUN': 0,\n",
        "        'VERB': 0,\n",
        "        'ADJ': 0,\n",
        "        'ADV': 0,\n",
        "    }\n",
        "    for token in doc:\n",
        "        if token.pos_ in pos_counts:\n",
        "            pos_counts[token.pos_] += 1\n",
        "\n",
        "    total_words = len(doc) if len(doc) > 0 else 1  # Avoid division by zero\n",
        "    for key in pos_counts:\n",
        "        pos_counts[key] /= total_words\n",
        "\n",
        "    return [pos_counts['NOUN'], pos_counts['VERB'], pos_counts['ADJ'], pos_counts['ADV']]\n",
        "\n",
        "def add_pos_features(features_file, merged_data_file):\n",
        "    # Load the existing features DataFrame\n",
        "    features_df = pd.read_csv(features_file)\n",
        "\n",
        "    # Load the merged DataFrame\n",
        "    merged_data = pd.read_csv(merged_data_file)\n",
        "\n",
        "    # Extract features and labels from the merged DataFrame\n",
        "    grouped = merged_data.groupby(['file_id'])\n",
        "\n",
        "    # Initialize lists to store new feature columns\n",
        "    pos_density_list = [None] * len(features_df)\n",
        "    prev_pos_density_list = [None] * len(features_df)\n",
        "\n",
        "    # Iterate over grouped data to calculate pos_density and prev_pos_density\n",
        "    for file_id, group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                pos_density_vector = calculate_pos_density(para)\n",
        "                prev_pos_density_vector = calculate_pos_density(prev_para)\n",
        "\n",
        "                # Find the corresponding index in the features_df\n",
        "                index = features_df[\n",
        "                    (features_df['file_id'] == group['file_id'].iloc[i]) &\n",
        "                    (features_df['paragraph_index'] == group['paragraph_index'].iloc[i])\n",
        "                ].index[0]\n",
        "\n",
        "                pos_density_list[index] = pos_density_vector\n",
        "                prev_pos_density_list[index] = prev_pos_density_vector\n",
        "\n",
        "    # Add the new features to the existing DataFrame\n",
        "    features_df['pos_density'] = pos_density_list\n",
        "    features_df['prev_pos_density'] = prev_pos_density_list\n",
        "\n",
        "    # Save the updated features DataFrame\n",
        "    features_df.to_csv(features_file, index=False)\n",
        "\n",
        "    print(f\"Features updated and saved in {features_file}\")\n",
        "\n",
        "# Example usage\n",
        "features_file = './pan24-multi-author-analysis/validation/easy_validation_features.csv'\n",
        "merged_data_file = './pan24-multi-author-analysis/easy/easy_validation_dataframe.csv'\n",
        "add_pos_features(features_file, merged_data_file)\n"
      ],
      "metadata": {
        "id": "xVf6GwgNdsyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_file2 = './pan24-multi-author-analysis/validation/medium_validation_features.csv'\n",
        "merged_data_file2 = './pan24-multi-author-analysis/medium/medium_validation_dataframe.csv'\n",
        "add_pos_features(features_file2, merged_data_file2)\n",
        "\n",
        "features_file3 = './pan24-multi-author-analysis/validation/hard_validation_features.csv'\n",
        "merged_data_file3 = './pan24-multi-author-analysis/hard/hard_validation_dataframe.csv'\n",
        "add_pos_features(features_file3, merged_data_file3)\n"
      ],
      "metadata": {
        "id": "i0efIBsPdxKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download punkt tokenizer for nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "\n",
        "def add_new_features(features_file, merged_data_file):\n",
        "    # Load the existing features DataFrame\n",
        "    features_df = pd.read_csv(features_file)\n",
        "\n",
        "    # Load the merged DataFrame\n",
        "    merged_data = pd.read_csv(merged_data_file)\n",
        "\n",
        "    # Extract features and labels from the merged DataFrame\n",
        "    grouped = merged_data.groupby(['file_id'])\n",
        "\n",
        "    # Initialize lists to store new feature columns\n",
        "    function_word_frequencies_list = [None] * len(features_df)\n",
        "    vocabulary_diversity_list = [None] * len(features_df)\n",
        "    sentence_length_variety_list = [None] * len(features_df)\n",
        "    prev_function_word_frequencies_list = [None] * len(features_df)\n",
        "    prev_vocabulary_diversity_list = [None] * len(features_df)\n",
        "    prev_sentence_length_variety_list = [None] * len(features_df)\n",
        "\n",
        "    # Iterate over grouped data to calculate new features\n",
        "    for file_id, group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                function_word_frequencies = calculate_function_word_frequencies(para)\n",
        "                vocabulary_diversity = calculate_vocabulary_diversity(para)\n",
        "                sentence_length_variety = calculate_sentence_length_variety(para)\n",
        "\n",
        "                prev_function_word_frequencies = calculate_function_word_frequencies(prev_para)\n",
        "                prev_vocabulary_diversity = calculate_vocabulary_diversity(prev_para)\n",
        "                prev_sentence_length_variety = calculate_sentence_length_variety(prev_para)\n",
        "\n",
        "                # Find the corresponding index in the features_df\n",
        "                index = features_df[\n",
        "                    (features_df['file_id'] == group['file_id'].iloc[i]) &\n",
        "                    (features_df['paragraph_index'] == group['paragraph_index'].iloc[i])\n",
        "                ].index[0]\n",
        "\n",
        "                function_word_frequencies_list[index] = function_word_frequencies\n",
        "                vocabulary_diversity_list[index] = vocabulary_diversity\n",
        "                sentence_length_variety_list[index] = sentence_length_variety\n",
        "                prev_function_word_frequencies_list[index] = prev_function_word_frequencies\n",
        "                prev_vocabulary_diversity_list[index] = prev_vocabulary_diversity\n",
        "                prev_sentence_length_variety_list[index] = prev_sentence_length_variety\n",
        "\n",
        "    # Add the new features to the existing DataFrame\n",
        "    features_df['function_word_frequencies'] = function_word_frequencies_list\n",
        "    features_df['vocabulary_diversity'] = vocabulary_diversity_list\n",
        "    features_df['sentence_length_variety'] = sentence_length_variety_list\n",
        "    features_df['prev_function_word_frequencies'] = prev_function_word_frequencies_list\n",
        "    features_df['prev_vocabulary_diversity'] = prev_vocabulary_diversity_list\n",
        "    features_df['prev_sentence_length_variety'] = prev_sentence_length_variety_list\n",
        "\n",
        "    # Save the updated features DataFrame\n",
        "    features_df.to_csv(features_file, index=False)\n",
        "\n",
        "    print(f\"Features updated and saved in {features_file}\")\n",
        "\n",
        "# Example usage\n",
        "features_file = './pan24-multi-author-analysis/validation/easy_validation_features.csv'\n",
        "merged_data_file = './pan24-multi-author-analysis/easy/easy_validation_dataframe.csv'\n",
        "add_new_features(features_file, merged_data_file)\n"
      ],
      "metadata": {
        "id": "oOPy36kYeWZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_file2 = './pan24-multi-author-analysis/validation/medium_validation_features.csv'\n",
        "merged_data_file2 = './pan24-multi-author-analysis/medium/medium_validation_dataframe.csv'\n",
        "add_new_features(features_file2, merged_data_file2)\n",
        "\n",
        "features_file3 = './pan24-multi-author-analysis/validation/hard_validation_features.csv'\n",
        "merged_data_file3 = './pan24-multi-author-analysis/hard/hard_validation_dataframe.csv'\n",
        "add_new_features(features_file3, merged_data_file3)"
      ],
      "metadata": {
        "id": "pNCFhXIbeaYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download punkt tokenizer for nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "\n",
        "def add_dependency_features(features_file, merged_data_file):\n",
        "    # Load the existing features DataFrame\n",
        "    features_df = pd.read_csv(features_file)\n",
        "\n",
        "    # Load the merged DataFrame\n",
        "    merged_data = pd.read_csv(merged_data_file)\n",
        "\n",
        "    # Extract features from the merged DataFrame\n",
        "    grouped = merged_data.groupby(['file_id'])\n",
        "\n",
        "    # Initialize lists to store new feature columns\n",
        "    dependency_list = [None] * len(features_df)\n",
        "    prev_dependency_list = [None] * len(features_df)\n",
        "\n",
        "    # Iterate over grouped data to calculate dependency features\n",
        "    for file_id, group in grouped:\n",
        "        for i in range(1, len(group)):\n",
        "            para = group['text'].iloc[i]\n",
        "            prev_para = group['text'].iloc[i-1]\n",
        "\n",
        "            if para.strip() and prev_para.strip():\n",
        "                dependency_features = calculate_dependency_parsing_features(para)\n",
        "                prev_dependency_features = calculate_dependency_parsing_features(prev_para)\n",
        "\n",
        "                # Find the corresponding index in the features_df\n",
        "                index = features_df[\n",
        "                    (features_df['file_id'] == group['file_id'].iloc[i]) &\n",
        "                    (features_df['paragraph_index'] == group['paragraph_index'].iloc[i])\n",
        "                ].index[0]\n",
        "\n",
        "                dependency_list[index] = dependency_features\n",
        "                prev_dependency_list[index] = prev_dependency_features\n",
        "\n",
        "    # Add the new features to the existing DataFrame\n",
        "    features_df['dependency'] = dependency_list\n",
        "    features_df['prev_dependency'] = prev_dependency_list\n",
        "\n",
        "    # Save the updated features DataFrame\n",
        "    features_df.to_csv(features_file, index=False)\n",
        "\n",
        "    print(f\"Features updated and saved in {features_file}\")\n",
        "\n",
        "features_file = './pan24-multi-author-analysis/validation/easy_validation_features.csv'\n",
        "merged_data_file = './pan24-multi-author-analysis/easy/easy_validation_dataframe.csv'\n",
        "add_dependency_features(features_file, merged_data_file)\n"
      ],
      "metadata": {
        "id": "QG7FOmArff7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_file2 = './pan24-multi-author-analysis/validation/medium_validation_features.csv'\n",
        "merged_data_file2 = './pan24-multi-author-analysis/medium/medium_validation_dataframe.csv'\n",
        "add_dependency_features(features_file2, merged_data_file2)\n",
        "\n",
        "features_file3 = './pan24-multi-author-analysis/validation/hard_validation_features.csv'\n",
        "merged_data_file3 = './pan24-multi-author-analysis/hard/hard_validation_dataframe.csv'\n",
        "add_dependency_features(features_file3, merged_data_file3)"
      ],
      "metadata": {
        "id": "yd6cgHnnfirN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Standardization"
      ],
      "metadata": {
        "id": "PTYj7gji6pfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def standardize_features(file_path, features_to_standardize):\n",
        "    \"\"\"\n",
        "    标准化CSV文件中的指定特征，并覆盖原数据。\n",
        "\n",
        "    :param file_path: CSV文件路径\n",
        "    :param features_to_standardize: 需要标准化的特征列表\n",
        "    \"\"\"\n",
        "    # 加载数据\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 初始化标准化器\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # 对指定特征进行标准化\n",
        "    data[features_to_standardize] = scaler.fit_transform(data[features_to_standardize])\n",
        "\n",
        "    # 将数据保存回 CSV 文件\n",
        "    data.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Features standardized and saved in {file_path}\")\n",
        "\n",
        "features_to_standardize = [\n",
        "    'clause_density', 'syllables_per_word', 'sentence_length', 'sentence_length_variety',\n",
        "    'prev_clause_density', 'prev_syllables_per_word', 'prev_sentence_length', 'prev_sentence_length_variety'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "#validation dataset\n",
        "file_path1 = './pan24-multi-author-analysis/validation/easy_validation_features.csv'\n",
        "standardize_features(file_path1, features_to_standardize)\n",
        "print(\"finish!\")\n",
        "\n",
        "file_path2 = './pan24-multi-author-analysis/validation/medium_validation_features.csv'\n",
        "standardize_features(file_path2, features_to_standardize)\n",
        "print(\"finish!\")\n",
        "\n",
        "file_path3 = './pan24-multi-author-analysis/validation/hard_validation_features.csv'\n",
        "standardize_features(file_path3, features_to_standardize)\n",
        "print(\"finish!\")"
      ],
      "metadata": {
        "id": "RJXBV06C6sW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 calculate absolute value differences between the same features for consecutive paragraphs as the training feature"
      ],
      "metadata": {
        "id": "WPCTIHza68NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "training dataset"
      ],
      "metadata": {
        "id": "daqjwYRaBH9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the ultimate merged features DataFrame\n",
        "merged_df = pd.read_csv('train_merged_features.csv')\n",
        "\n",
        "# Ensure pos_features and prev_pos_features are evaluated as lists\n",
        "merged_df['pos_features'] = merged_df['pos_features'].apply(eval)\n",
        "merged_df['prev_pos_features'] = merged_df['prev_pos_features'].apply(eval)\n",
        "merged_df['function_word_frequencies'] = merged_df['function_word_frequencies'].apply(eval)\n",
        "merged_df['prev_function_word_frequencies'] = merged_df['prev_function_word_frequencies'].apply(eval)\n",
        "merged_df['dependency'] = merged_df['dependency'].apply(eval)\n",
        "merged_df['prev_dependency'] = merged_df['prev_dependency'].apply(eval)\n",
        "\n",
        "# Calculate the absolute differences for each feature\n",
        "def calculate_absolute_differences(df):\n",
        "    differences = pd.DataFrame()\n",
        "    differences['file_id'] = df['file_id']\n",
        "    differences['paragraph_index'] = df['paragraph_index']\n",
        "    differences['difficulty'] = df['difficulty']\n",
        "\n",
        "    # Calculate absolute differences for individual features\n",
        "    for feature in ['clause_density', 'punctuation_density', 'syllables_per_word', 'sentence_length', 'noun_complexity']:\n",
        "        differences[f'{feature}_diff'] = (df[feature] - df[f'prev_{feature}']).abs()\n",
        "\n",
        "    # Calculate absolute differences for function word frequencies\n",
        "    differences['function_word_diff'] = df.apply(lambda row: [abs(x - y) for x, y in zip(row['function_word_frequencies'], row['prev_function_word_frequencies'])], axis=1)\n",
        "\n",
        "    # Calculate absolute differences for vocabulary diversity and sentence length variety\n",
        "    differences['vocabulary_diversity_diff'] = (df['vocabulary_diversity'] - df['prev_vocabulary_diversity']).abs()\n",
        "    differences['sentence_length_variety_diff'] = (df['sentence_length_variety'] - df['prev_sentence_length_variety']).abs()\n",
        "\n",
        "    # Calculate absolute differences for dependency features\n",
        "    differences['dependency_diff'] = df.apply(lambda row: [abs(x - y) for x, y in zip(row['dependency'], row['prev_dependency'])], axis=1)\n",
        "\n",
        "    # Calculate absolute differences for POS features\n",
        "    differences['pos_features_diff'] = df.apply(lambda row: [abs(x - y) for x, y in zip(row['pos_features'], row['prev_pos_features'])], axis=1)\n",
        "\n",
        "    return differences\n",
        "\n",
        "# Calculate the absolute differences DataFrame\n",
        "absolute_differences_df = calculate_absolute_differences(merged_df)\n",
        "\n",
        "# Save the new training feature DataFrame\n",
        "absolute_differences_df.to_csv('training_features_absolute.csv', index=False)\n",
        "\n",
        "print(\"Training features saved\")\n"
      ],
      "metadata": {
        "id": "PksoIWTH7Frv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation dataset"
      ],
      "metadata": {
        "id": "k4RuFuLOBKmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_absolute_differences(df):\n",
        "    \"\"\"\n",
        "    Calculate the absolute differences for specified features between current and previous paragraphs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame containing the features and their previous values.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame containing the absolute differences for each feature.\n",
        "    \"\"\"\n",
        "    differences = pd.DataFrame()\n",
        "    differences['file_id'] = df['file_id']\n",
        "    differences['paragraph_index'] = df['paragraph_index']\n",
        "\n",
        "    # Calculate absolute differences for individual features\n",
        "    for feature in ['clause_density', 'punctuation_density', 'syllables_per_word', 'sentence_length', 'noun_complexity']:\n",
        "        differences[f'{feature}_diff'] = (df[feature] - df[f'prev_{feature}']).abs()\n",
        "\n",
        "    # Calculate absolute differences for function word frequencies\n",
        "    if 'function_word_frequencies' in df.columns and 'prev_function_word_frequencies' in df.columns:\n",
        "        differences['function_word_diff'] = df.apply(lambda row: [abs(x - y) for x, y in zip(row['function_word_frequencies'], row['prev_function_word_frequencies'])], axis=1)\n",
        "\n",
        "    # Calculate absolute differences for vocabulary diversity and sentence length variety\n",
        "    if 'vocabulary_diversity' in df.columns and 'prev_vocabulary_diversity' in df.columns:\n",
        "        differences['vocabulary_diversity_diff'] = (df['vocabulary_diversity'] - df['prev_vocabulary_diversity']).abs()\n",
        "    if 'sentence_length_variety' in df.columns and 'prev_sentence_length_variety' in df.columns:\n",
        "        differences['sentence_length_variety_diff'] = (df['sentence_length_variety'] - df['prev_sentence_length_variety']).abs()\n",
        "\n",
        "    # Calculate absolute differences for dependency features\n",
        "    if 'dependency' in df.columns and 'prev_dependency' in df.columns:\n",
        "        differences['dependency_diff'] = df.apply(lambda row: [abs(x - y) for x, y in zip(row['dependency'], row['prev_dependency'])], axis=1)\n",
        "\n",
        "    # Calculate absolute differences for POS features\n",
        "    if 'pos_features' in df.columns and 'prev_pos_features' in df.columns:\n",
        "        differences['pos_features_diff'] = df.apply(lambda row: [abs(x - y) for x, y in zip(row['pos_features'], row['prev_pos_features'])], axis=1)\n",
        "\n",
        "    return differences\n",
        "\n",
        "def process_file(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Load the input file, calculate absolute differences for features, and save the result to the output file.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to the input CSV file.\n",
        "    output_file (str): Path to the output CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the merged features DataFrame\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Rename columns\n",
        "        if 'pos_density' in df.columns:\n",
        "            df.rename(columns={'pos_density': 'pos_features'}, inplace=True)\n",
        "        if 'prev_pos_density' in df.columns:\n",
        "            df.rename(columns={'prev_pos_density': 'prev_pos_features'}, inplace=True)\n",
        "\n",
        "        # Ensure list-like columns are evaluated correctly if they exist\n",
        "        for col in ['pos_features', 'prev_pos_features', 'function_word_frequencies', 'prev_function_word_frequencies', 'dependency', 'prev_dependency']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].apply(eval)\n",
        "\n",
        "        # Calculate the absolute differences\n",
        "        differences_df = calculate_absolute_differences(df)\n",
        "\n",
        "        # Save the new DataFrame to CSV\n",
        "        differences_df.to_csv(output_file, index=False)\n",
        "        print(f\"Training features saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {input_file}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage for multiple files\n",
        "files_to_process = [\n",
        "    ('./pan24-multi-author-analysis/validation/easy_validation_features.csv', './pan24-multi-author-analysis/validation/easy_validation_features_difference.csv'),\n",
        "    ('./pan24-multi-author-analysis/validation/medium_validation_features.csv', './pan24-multi-author-analysis/validation/medium_validation_features_difference.csv'),\n",
        "    ('./pan24-multi-author-analysis/validation/hard_validation_features.csv', './pan24-multi-author-analysis/validation/hard_validation_features_difference.csv')\n",
        "    # Add more files as needed\n",
        "]\n",
        "\n",
        "for input_file, output_file in files_to_process:\n",
        "    process_file(input_file, output_file)\n"
      ],
      "metadata": {
        "id": "y2X83jYRBMbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Model Training"
      ],
      "metadata": {
        "id": "BmNuMg8TFvGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Logistic regression model"
      ],
      "metadata": {
        "id": "gAo6a3qCdyBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Define file paths for training set\n",
        "train_features_path = 'trainset/training_features_absolute.csv'\n",
        "train_labels_path = 'trainset/combined_train_labels.csv'\n",
        "\n",
        "# Define file paths for validation sets\n",
        "val_easy_features_path = 'valset/easy_validation_features.csv'\n",
        "val_easy_labels_path = 'valset/easy_validation_labels.csv'\n",
        "\n",
        "val_medium_features_path = 'valset/medium_validation_features.csv'\n",
        "val_medium_labels_path = 'valset/medium_validation_labels.csv'\n",
        "\n",
        "val_hard_features_path = 'valset/hard_validation_features.csv'\n",
        "val_hard_labels_path = 'valset/hard_validation_labels.csv'\n",
        "\n",
        "# Function to convert string representations of lists to actual lists\n",
        "def convert_string_lists(df, columns):\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n",
        "    return df\n",
        "\n",
        "# Function to flatten list columns into separate columns\n",
        "def flatten_columns(df, columns):\n",
        "    for col in columns:\n",
        "        # Create new columns for each element in the list\n",
        "        list_col_df = pd.DataFrame(df[col].tolist(), index=df.index)\n",
        "        list_col_df = list_col_df.add_prefix(f'{col}_')\n",
        "        df = df.drop(col, axis=1)\n",
        "        df = pd.concat([df, list_col_df], axis=1)\n",
        "    return df\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model_performance(model, X_val, y_val, dataset_name):\n",
        "    y_pred = model.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(f'\\nPerformance on {dataset_name} Validation Set:')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "\n",
        "# Load your training set features and labels\n",
        "train_features_df = pd.read_csv(train_features_path)\n",
        "train_labels_df = pd.read_csv(train_labels_path)\n",
        "\n",
        "# Columns containing lists stored as strings\n",
        "list_columns = [\n",
        "    'function_word_diff', 'dependency_diff', 'pos_features_diff'\n",
        "]\n",
        "\n",
        "# Convert string lists to actual lists and flatten them\n",
        "train_features_df = convert_string_lists(train_features_df, list_columns)\n",
        "train_features_df = flatten_columns(train_features_df, list_columns)\n",
        "\n",
        "# Extract features and labels for training set\n",
        "X_train = train_features_df.drop(['file_id', 'paragraph_index', 'difficulty', 'noun_complexity_diff'], axis=1)\n",
        "y_train = train_labels_df['author_change']\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on Easy Validation Set\n",
        "val_easy_features_df = pd.read_csv(val_easy_features_path)\n",
        "val_easy_labels_df = pd.read_csv(val_easy_labels_path)\n",
        "val_easy_features_df = convert_string_lists(val_easy_features_df, list_columns)\n",
        "val_easy_features_df = flatten_columns(val_easy_features_df, list_columns)\n",
        "X_val_easy = val_easy_features_df.drop(['file_id', 'paragraph_index', 'noun_complexity_diff'], axis=1)\n",
        "y_val_easy = val_easy_labels_df['author_change']\n",
        "\n",
        "evaluate_model_performance(log_reg, X_val_easy, y_val_easy, \"Easy (Logistic Regression)\")\n",
        "\n",
        "# Evaluate on Medium Validation Set\n",
        "val_medium_features_df = pd.read_csv(val_medium_features_path)\n",
        "val_medium_labels_df = pd.read_csv(val_medium_labels_path)\n",
        "val_medium_features_df = convert_string_lists(val_medium_features_df, list_columns)\n",
        "val_medium_features_df = flatten_columns(val_medium_features_df, list_columns)\n",
        "X_val_medium = val_medium_features_df.drop(['file_id', 'paragraph_index','noun_complexity_diff'], axis=1)\n",
        "y_val_medium = val_medium_labels_df['author_change']\n",
        "\n",
        "evaluate_model_performance(log_reg, X_val_medium, y_val_medium, \"Medium (Logistic Regression)\")\n",
        "\n",
        "# Evaluate on Hard Validation Set\n",
        "val_hard_features_df = pd.read_csv(val_hard_features_path)\n",
        "val_hard_labels_df = pd.read_csv(val_hard_labels_path)\n",
        "val_hard_features_df = convert_string_lists(val_hard_features_df, list_columns)\n",
        "val_hard_features_df = flatten_columns(val_hard_features_df, list_columns)\n",
        "X_val_hard = val_hard_features_df.drop(['file_id', 'paragraph_index', 'noun_complexity_diff'], axis=1)\n",
        "y_val_hard = val_hard_labels_df['author_change']\n",
        "\n",
        "evaluate_model_performance(log_reg, X_val_hard, y_val_hard, \"Hard (Logistic Regression)\")\n",
        "\n",
        "# Calculate feature importance\n",
        "feature_names = X_train.columns\n",
        "coefficients = log_reg.coef_[0]\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Display feature importance\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "id": "-cVHo8OId2Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 random forest model\n"
      ],
      "metadata": {
        "id": "HXWWJNGKHRgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.1 baseline random forest model"
      ],
      "metadata": {
        "id": "cyWWW6e0eJPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Define file paths for training set\n",
        "train_features_path = '/content/drive/MyDrive/lp/dataset/training/training_features_absolute.csv'\n",
        "train_labels_path = '/content/drive/MyDrive/lp/dataset/training/combined_train_labels.csv'\n",
        "\n",
        "# Define file paths for validation sets\n",
        "val_easy_features_path = '/content/drive/MyDrive/lp/dataset/validation/easy_validation_features_difference.csv'\n",
        "val_easy_labels_path = '/content/drive/MyDrive/lp/dataset/validation/easy_validation_labels.csv'\n",
        "\n",
        "val_medium_features_path = '/content/drive/MyDrive/lp/dataset/validation/medium_validation_features_difference.csv'\n",
        "val_medium_labels_path = '/content/drive/MyDrive/lp/dataset/validation/medium_validation_labels.csv'\n",
        "\n",
        "val_hard_features_path = '/content/drive/MyDrive/lp/dataset/validation/hard_validation_features_difference.csv'\n",
        "val_hard_labels_path = '/content/drive/MyDrive/lp/dataset/validation/hard_validation_labels.csv'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cyOdlj8wg8Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "# Function to convert string representations of lists to actual numpy arrays\n",
        "def convert_string_to_numpy(df, columns):\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(lambda x: np.array(ast.literal_eval(x)))\n",
        "    return df\n",
        "\n",
        "# Function to combine vector columns into single columns\n",
        "def combine_vector_columns(df, columns):\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
        "    return df\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model_performance(model, X_val, y_val, dataset_name):\n",
        "    y_pred = model.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(f'\\nPerformance on {dataset_name} Validation Set:')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "\n",
        "# Load your training set features and labels\n",
        "train_features_df = pd.read_csv(train_features_path)\n",
        "train_labels_df = pd.read_csv(train_labels_path)\n",
        "\n",
        "# Columns containing lists stored as strings\n",
        "list_columns = [\n",
        "    'function_word_diff', 'dependency_diff', 'pos_features_diff'\n",
        "]\n",
        "\n",
        "# Convert string lists to numpy arrays and combine them\n",
        "train_features_df = convert_string_to_numpy(train_features_df, list_columns)\n",
        "train_features_df = combine_vector_columns(train_features_df, list_columns)\n",
        "\n",
        "# Extract features and labels for training set\n",
        "X_train = train_features_df.drop(['file_id', 'paragraph_index', 'difficulty', 'noun_complexity_diff'], axis=1)\n",
        "y_train = train_labels_df['author_change']\n",
        "\n",
        "# write X_train and y_train into new CSV files\n",
        "X_train.to_csv('/content/drive/MyDrive/lp/dataset/training/X_train.csv', index=False)\n",
        "y_train.to_csv('/content/drive/MyDrive/lp/dataset/training/y_train.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate on Easy Validation Set\n",
        "val_easy_features_df = pd.read_csv(val_easy_features_path)\n",
        "val_easy_labels_df = pd.read_csv(val_easy_labels_path)\n",
        "val_easy_features_df = convert_string_to_numpy(val_easy_features_df, list_columns)\n",
        "val_easy_features_df = combine_vector_columns(val_easy_features_df, list_columns)\n",
        "X_val_easy = val_easy_features_df.drop(['file_id', 'paragraph_index', 'noun_complexity_diff'], axis=1)\n",
        "y_val_easy = val_easy_labels_df['author_change']\n",
        "X_val_easy.to_csv('/content/drive/MyDrive/lp/dataset/validation/X_val_easy.csv', index=False)\n",
        "y_val_easy.to_csv('/content/drive/MyDrive/lp/dataset/validation/y_val_easy.csv', index=False)\n",
        "\n",
        "# Evaluate on Medium Validation Set\n",
        "val_medium_features_df = pd.read_csv(val_medium_features_path)\n",
        "val_medium_labels_df = pd.read_csv(val_medium_labels_path)\n",
        "val_medium_features_df = convert_string_to_numpy(val_medium_features_df, list_columns)\n",
        "val_medium_features_df = combine_vector_columns(val_medium_features_df, list_columns)\n",
        "X_val_medium = val_medium_features_df.drop(['file_id', 'paragraph_index', 'noun_complexity_diff'], axis=1)\n",
        "y_val_medium = val_medium_labels_df['author_change']\n",
        "X_val_medium.to_csv('/content/drive/MyDrive/lp/dataset/validation/X_val_medium.csv', index=False)\n",
        "y_val_medium.to_csv('/content/drive/MyDrive/lp/dataset/validation/y_val_medium.csv', index=False)\n",
        "\n",
        "# Evaluate on Hard Validation Set\n",
        "val_hard_features_df = pd.read_csv(val_hard_features_path)\n",
        "val_hard_labels_df = pd.read_csv(val_hard_labels_path)\n",
        "val_hard_features_df = convert_string_to_numpy(val_hard_features_df, list_columns)\n",
        "val_hard_features_df = combine_vector_columns(val_hard_features_df, list_columns)\n",
        "X_val_hard = val_hard_features_df.drop(['file_id', 'paragraph_index', 'noun_complexity_diff'], axis=1)\n",
        "y_val_hard = val_hard_labels_df['author_change']\n",
        "X_val_hard.to_csv('/content/drive/MyDrive/lp/dataset/validation/X_val_hard.csv', index=False)\n",
        "y_val_hard.to_csv('/content/drive/MyDrive/lp/dataset/validation/y_val_hard.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ke7xd7u1g8Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "evaluate_model_performance(rf_model, X_val_easy, y_val_easy, \"Easy (Random Forest)\")\n",
        "evaluate_model_performance(rf_model, X_val_medium, y_val_medium, \"Medium (Random Forest)\")\n",
        "evaluate_model_performance(rf_model, X_val_hard, y_val_hard, \"Hard (Random Forest)\")\n",
        "\n",
        "# Feature Importances from Random Forest\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print('\\nFeature Importances:')\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "fZklTOINg8Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bayesian-optimization"
      ],
      "metadata": {
        "id": "oyQUybyChI_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Counter to track the number of times the model is trained\n",
        "training_counter = 0\n",
        "\n",
        "# Define the hyperparameter space to search\n",
        "def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n",
        "    global training_counter\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=int(n_estimators),\n",
        "        max_depth=int(max_depth),\n",
        "        min_samples_split=int(min_samples_split),\n",
        "        min_samples_leaf=int(min_samples_leaf),\n",
        "        max_features=max_features,\n",
        "        random_state=42\n",
        "    )\n",
        "    # Perform cross-validation and return the mean F1 score\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1')  # Reduced CV folds to 3\n",
        "    training_counter += 3  # Increase by the number of CV folds\n",
        "    return np.mean(cv_scores)\n",
        "\n",
        "# Set up the Bayesian Optimizer\n",
        "optimizer = BayesianOptimization(\n",
        "    f=rf_cv,\n",
        "    pbounds={\n",
        "        'n_estimators': (10, 200),\n",
        "        'max_depth': (1, 50),\n",
        "        'min_samples_split': (2, 10),\n",
        "        'min_samples_leaf': (1, 10),\n",
        "        'max_features': (0.1, 0.999)\n",
        "    },\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Perform optimization with reduced init_points and n_iter\n",
        "optimizer.maximize(init_points=10, n_iter=20)  # Reduced init_points to 5 and n_iter to 10\n",
        "\n",
        "print(f\"Total number of model trainings: {training_counter}\")\n",
        "\n",
        "# Retrieve the best parameters\n",
        "best_params = optimizer.max['params']\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "best_params['max_depth'] = int(best_params['max_depth'])\n",
        "best_params['min_samples_split'] = int(best_params['min_samples_split'])\n",
        "best_params['min_samples_leaf'] = int(best_params['min_samples_leaf'])\n",
        "\n",
        "print(\"Best Parameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# Train Random Forest with best parameters\n",
        "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model_performance(model, X_val, y_val, dataset_name):\n",
        "    y_pred = model.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(f'\\nPerformance on {dataset_name} Validation Set:')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "\n",
        "# Evaluate on Easy Validation Set\n",
        "evaluate_model_performance(best_rf_model, X_val_easy, y_val_easy, \"Easy (Optimized Random Forest)\")\n",
        "\n",
        "# Evaluate on Medium Validation Set\n",
        "evaluate_model_performance(best_rf_model, X_val_medium, y_val_medium, \"Medium (Optimized Random Forest)\")\n",
        "\n",
        "# Evaluate on Hard Validation Set\n",
        "evaluate_model_performance(best_rf_model, X_val_hard, y_val_hard, \"Hard (Optimized Random Forest)\")\n"
      ],
      "metadata": {
        "id": "LnFo-idmg8Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest with best parameters\n",
        "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model_performance(model, X_val, y_val, dataset_name):\n",
        "    y_pred = model.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(f'\\nPerformance on {dataset_name} Validation Set:')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "\n",
        "# Evaluate on Easy Validation Set\n",
        "evaluate_model_performance(best_rf_model, X_val_easy, y_val_easy, \"Easy (Optimized Random Forest)\")\n",
        "\n",
        "# Evaluate on Medium Validation Set\n",
        "evaluate_model_performance(best_rf_model, X_val_medium, y_val_medium, \"Medium (Optimized Random Forest)\")\n",
        "\n",
        "# Evaluate on Hard Validation Set\n",
        "evaluate_model_performance(best_rf_model, X_val_hard, y_val_hard, \"Hard (Optimized Random Forest)\")"
      ],
      "metadata": {
        "id": "3PIa4Lxcg8JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importances from Random Forest\n",
        "importances = best_rf_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print('\\nFeature Importances:')\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "id": "HR9-D72eg8HK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}